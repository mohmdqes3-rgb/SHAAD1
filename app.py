import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import cv2
import mediapipe as mp
import numpy as np

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Ù†Ø¸Ø§Ù… ØªØ±Ø¬Ù…Ø© Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø©", layout="wide")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ (Ø¶Ø±ÙˆØ±ÙŠØ© Ù„Ø¹Ù…Ù„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙÙŠ Ø§Ù„Ø³ÙŠØ±ÙØ±)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# --- Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª ---
class GestureEngine:
    def __init__(self):
        self.rules = []
        self._register()

    def _features(self, landmarks, handed):
        pts = [np.array([p.x, p.y, p.z], dtype=np.float32) for p in landmarks]
        wrist, idx_mcp, pnk_mcp = pts[0], pts[5], pts[17]
        v1, v2 = idx_mcp - wrist, pnk_mcp - wrist
        palm_normal = np.cross(v1, v2)
        
        facing = "Ø§Ù…Ø§Ù…" if palm_normal[2] < -0.12 else "Ø®Ù„Ù" if palm_normal[2] > 0.12 else "Ù…Ø­Ø§ÙŠØ¯"

        def is_open(f): return pts[f[3]][1] < pts[f[1]][1]
        
        fingers = {
            "Ø§Ø¨Ù‡Ø§Ù…": pts[4][0] < pts[3][0] if handed == "Left" else pts[4][0] > pts[3][0],
            "Ø³Ø¨Ø§Ø¨Ø©": is_open([5,6,7,8]), "ÙˆØ³Ø·Ù‰": is_open([9,10,11,12]),
            "Ø¨Ù†ØµØ±": is_open([13,14,15,16]), "Ø®Ù†ØµØ±": is_open([17,18,19,20])
        }
        return {"open": fingers, "facing": facing}

    def _register(self):
        self.rules.append(("Ø³Ù„Ø§Ù…", lambda f: all(f["open"].values())))
        self.rules.append(("ØªÙˆÙ‚Ù", lambda f: f["facing"]=="Ø§Ù…Ø§Ù…" and f["open"]["Ø³Ø¨Ø§Ø¨Ø©"]))
        self.rules.append(("Ù†ØµØ±", lambda f: f["open"]["Ø³Ø¨Ø§Ø¨Ø©"] and f["open"]["ÙˆØ³Ø·Ù‰"] and not f["open"]["Ø¨Ù†ØµØ±"]))
        self.rules.append(("Ø£Ù†Ø§", lambda f: f["open"]["Ø³Ø¨Ø§Ø¨Ø©"] and not f["open"]["ÙˆØ³Ø·Ù‰"]))

    def classify(self, landmarks, handed):
        f = self._features(landmarks, handed)
        for n, fn in self.rules:
            try:
                if fn(f): return n
            except: continue
        return "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."

# --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ---
class VideoProcessor:
    def __init__(self):
        # ØªÙ… ØªØµØ­ÙŠØ­ __init__ Ù‡Ù†Ø§ Ù„Ø¶Ù…Ø§Ù† Ø¹Ù…Ù„Ù‡Ø§ ÙÙŠ Ø§Ù„Ø³ÙŠØ±ÙØ±
        self.hands = mp.solutions.hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.engine = GestureEngine()

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        res = self.hands.process(rgb)

        if res.multi_hand_landmarks:
            hand = res.multi_hand_landmarks[0]
            handed = res.multi_handedness[0].classification[0].label
            label = self.engine.classify(hand.landmark, handed)
            
            mp.solutions.drawing_utils.draw_landmarks(img, hand, mp.solutions.hands.HAND_CONNECTIONS)
            cv2.putText(img, label, (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)

        return frame.from_ndarray(img, format="bgr24")

# --- ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ---
st.sidebar.title("ğŸ“ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹")
st.sidebar.info("""
**Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù„Ø¨Ø§Øª:**
* Ø´Ù‡Ø¯ ØµØ§Ø¯Ù‚ Ø­Ù…Ø²Ø©
* Ø¨Ù†ÙŠÙ† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¹Ø¨Ø¯ Ø§Ù„Ø²Ù‡Ø±Ø©
* ÙØ§Ø·Ù…Ø© ÙƒØ±ÙŠÙ… Ø­Ù…ÙŠØ¯ Ø´Ø¨ÙŠØ¨

**Ø¨Ø¥Ø´Ø±Ø§Ù:**
* Ø§Ù„Ø³Øª Ø²Ù‡Ø±Ø§Ø¡ ÙƒØ§Ø¸Ù… ÙØ±Ù‡ÙˆØ¯
""")

st.title("âœ¨ Ù†Ø¸Ø§Ù… ØªØ±Ø¬Ù…Ø© Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© (AI)")

webrtc_streamer(
    key="sign-lang",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration=RTC_CONFIGURATION,
    video_processor_factory=VideoProcessor,
    async_processing=True,
    media_stream_constraints={"video": True, "audio": False},
)
